# note that python2 and python3 come installed by default with Ubuntu





### Anaconda ###

# Go to home directory
cd ~

# pick version of anaconda to download from (.sh file):
# https://repo.continuum.io/archive/

wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh

# install anaconda
bash Anaconda2-4.4.0-Linux-x86_64.sh -b -p ~/anaconda

# delete installer
rm Anaconda2-4.4.0-Linux-x86_64.sh

# save path
echo 'export PATH="~/anaconda/bin:$PATH"' >> ~/.bashrc

# Refresh basically
source .bashrc

# update anaconda
conda update conda

# check that installation was successful by launching jupyter
jupyter notebook

### Anaconda ###







### Java ###

# Step 1: Installing Java
# Check to see if Java is already installed by typing:

java -version

# If you see something like “The program ‘java’ can be found in the following packages…”
# It probably means you actually need to install Java.
# The easiest option for installing Java is using the version packaged with Ubuntu.
# First, update the package index by in your terminal typing:

sudo apt-get update

# After entering your password it will update some stuff.
# Now you can install the JDK with the following command:

sudo apt-get install default-jdk

# Continue with “Y”.

### Java ###






### Scala ###

# Step 2: Install Scala

sudo apt-get install scala

# Type scala into your terminal:

scala

# You should see the scala REPL running. Test it with:

println(“Hello World”)

# You can then quit the Scala REPL with

:q

### Scala ###






### Git ###

# install
sudo apt-get install git

# configure for use
git config --global user.name "your_username"
git config --global user.email "your_email"

git clone https://github.com/your_username/repository

### Git ###








### PySpark ###

# install pyspark
pip install pyspark

# edit bashrc file
gedit ~/.bashrc

# add to end of file
export SPARK_HOME=/home/fqaiser94/spark-2.2.0-bin-hadoop2.7
export PATH=$SPARK_HOME/bin:$PATH

# close file

# quit terminal
exit

# reopen terminal
# ctrl + alt + t

# try running pyspark
pyspark

### PySpark ###










### Spark ###

# Go to home directory
cd ~

# Download a pre-built for Hadoop 2.7 version of Spark (preferably Spark 2.0 or later)

wget http://apache.mirrors.ionfish.org/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz

tar xvf spark-2.2.0-bin-hadoop2.7.tgz

# (Your version numbers may differ).
# Then once its done extracting the Spark folder, use:

cd spark-2.2.0-bin-hadoop2.7

# then use

cd bin

# and then type

./spark-shell

# and you should see the spark shell pop up
# this is where you can load .scala scripts.
# You can confirm that this is working by typing something like:

println(“Spark shell is running”)

# That’s it! You should now have Spark running on your computer.

# add spark_home path

export SPARK_HOME=/opt/spark
export PATH=$SPARK_HOME/bin:$PATH

### Spark ###





### SBT ###

wget http://scalasbt.artifactoryonline.com/scalasbt/sbt-native-packages/org/scala-sbt/sbt//0.12.3/sbt.deb
sudo dpkg -i sbt.deb
sudo apt-get -y update
sudo apt-get -y install sbt

### SBT ###
