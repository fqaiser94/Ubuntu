







### Python setup ###

# python2 and python3 come installed by default with Ubuntu

# install pip
sudo apt install python3-pip

# upgrade pip to latest version
pip3 install --upgrade pip

# install python3-tk (dunno why this doesn't come with Ubuntu by default)
sudo apt-get install python3-tk

# install my favourite packages
# note the use of 'pip3' (which is applicable to python3)
# rather than 'pip' (which is applicable to python2)

# IDE
pip3 install jupyter --user

# data manipulation
pip3 install pandas

# data visualization
pip3 install ggplot --user
pip3 install missingno --user

# machine learning
pip3 install sklearn --user

# big data
pip3 install dask[complete] --user
sudo apt-get install graphviz


### Python setup ###








### Anaconda ###

# Go to home directory
cd ~

# pick version of anaconda to download from (.sh file):
# https://repo.continuum.io/archive/

wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh

# install anaconda
bash Anaconda2-4.4.0-Linux-x86_64.sh -b -p ~/anaconda

# delete installer
rm Anaconda2-4.4.0-Linux-x86_64.sh

# save path
echo 'export PATH="~/anaconda/bin:$PATH"' >> ~/.bashrc

# refresh basically
source .bashrc

# update anaconda
conda update conda

# check that installation was successful by launching jupyter
jupyter notebook

### Anaconda ###







### Jupyter ###

# Although Anaconda installs Jupyter
# the Python3 kernel is unavailable by default

conda create -n py35 python=3.5
source activate py35
conda install notebook ipykernel
ipython kernel install --user --name=python3.5

### Jupyter ###













### Java ###

# Step 1: Installing Java
# Check to see if Java is already installed by typing:

java -version

# If you see something like “The program ‘java’ can be found in the following packages…”
# It probably means you actually need to install Java.
# The easiest option for installing Java is using the version packaged with Ubuntu.
# First, update the package index by in your terminal typing:

sudo apt-get update

# After entering your password it will update some stuff.
# Now you can install the JDK with the following command:

sudo apt-get install default-jdk

# Continue with “Y”.

### Java ###






### Scala ###

# Step 2: Install Scala

sudo apt-get install scala

# Type scala into your terminal:

scala

# You should see the scala REPL running. Test it with:

println(“Hello World”)

# You can then quit the Scala REPL with

:q

### Scala ###






### Git ###

# install
sudo apt-get install git

# configure for use
git config --global user.name "your_username"
git config --global user.email "your_email"

# save credentials for an hour at a time
git config --global credential.helper 'cache --timeout=3600'

# clone a repository
git clone https://github.com/your_username/repository

### Git ###








### PySpark ###

# install pyspark
pip install pyspark

# edit bashrc file
gedit ~/.bashrc

# add to end of file
export SPARK_HOME=/home/fqaiser94/spark-2.2.0-bin-hadoop2.7
export PATH=$SPARK_HOME/bin:$PATH

# close file

# quit terminal
exit

# reopen terminal
# ctrl + alt + t

# try running pyspark
pyspark

### PySpark ###





### Launch spark in Jupyter ###

# this is a generalized way to use PySpark in a Jupyter Notebook
# uses findSpark package to make a Spark Context available in your code.
# findSpark package is not specific to Jupyter Notebook, you can use this trick in your favorite IDE too.

# install findspark:
pip install findspark

## Test to check if working ##

# launch a Jupyter Notebook:
jupyter notebook

# Create a new Python [default] notebook
# write the following script:

import findspark
findspark.init()
import pyspark
import random
sc = pyspark.SparkContext(appName="Pi")
num_samples = 100000000
def inside(p):
  x, y = random.random(), random.random()
  return x*x + y*y < 1
count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print(pi)
sc.stop()

# run script
# verify output is:
3.14107536









### Spark ###

# Go to home directory
cd ~

# Download a pre-built for Hadoop 2.7 version of Spark (preferably Spark 2.0 or later)

wget http://apache.mirrors.ionfish.org/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz

tar xvf spark-2.2.0-bin-hadoop2.7.tgz

# (Your version numbers may differ).
# Then once its done extracting the Spark folder, use:

cd spark-2.2.0-bin-hadoop2.7

# then use

cd bin

# and then type

./spark-shell

# and you should see the spark shell pop up
# this is where you can load .scala scripts.
# You can confirm that this is working by typing something like:

println(“Spark shell is running”)

# That’s it! You should now have Spark running on your computer.

# add spark_home path

export SPARK_HOME=/opt/spark
export PATH=$SPARK_HOME/bin:$PATH

### Spark ###





### SBT ###

wget http://scalasbt.artifactoryonline.com/scalasbt/sbt-native-packages/org/scala-sbt/sbt//0.12.3/sbt.deb
sudo dpkg -i sbt.deb
sudo apt-get -y update
sudo apt-get -y install sbt

### SBT ###
