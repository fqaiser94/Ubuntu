# note that python2 and python3 come installed by default with Ubuntu





### Anaconda ###

# Go to home directory
cd ~

# pick version of anaconda to download from (.sh file):  
# https://repo.continuum.io/archive/

wget https://repo.continuum.io/archive/Anaconda2-4.4.0-Linux-x86_64.sh

# install anaconda
bash Anaconda2-4.4.0-Linux-x86_64.sh -b -p ~/anaconda

# delete installer
rm Anaconda2-4.4.0-Linux-x86_64.sh

# save path
echo 'export PATH="~/anaconda/bin:$PATH"' >> ~/.bashrc 

# Refresh basically
source .bashrc

# update anaconda
conda update conda

# check that installation was successful by launching jupyter
jupyter notebook

### Anaconda ###







### Java ###

# Step 1: Installing Java
# Check to see if Java is already installed by typing:

java -version

# If you see something like “The program ‘java’ can be found in the following packages…”
# It probably means you actually need to install Java.
# The easiest option for installing Java is using the version packaged with Ubuntu.
# First, update the package index by in your terminal typing:

sudo apt-get update

# After entering your password it will update some stuff.
# Now you can install the JDK with the following command:

sudo apt-get install default-jdk

# Continue with “Y”.

### Java ###






### Scala ###

# Step 2: Install Scala

sudo apt-get install scala

# Type scala into your terminal:

scala

# You should see the scala REPL running. Test it with:

println(“Hello World”)

# You can then quit the Scala REPL with

:q

### Scala ###






### Git ###

sudo apt-get install git

### Git ###






### Spark ###

#Step 3: Install Spark
# Next its time to install Spark.

# Go to 																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																																
# Download a pre-built for Hadoop 2.7 version of Spark (preferably Spark 2.0 or later). 
# Download the .tgz file and remember where you save it on your computer.

# Then in your terminal change directory to where you saved that .tgz file (or just move the file to your home folder), then use: 

tar xvf spark-2.2.0-bin-hadoop2.7.tgz

# (Your version numbers may differ).
# Then once its done extracting the Spark folder, use:

cd spark-2.2.0-bin-hadoop2.7

# then use

cd bin

# and then type

./spark-shell

# and you should see the spark shell pop up
# this is where you can load .scala scripts. 
# You can confirm that this is working by typing something like:

println(“Spark shell is running”)

# That’s it! You should now have Spark running on your computer.

### Spark ###
